{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EmoKnob Demo - Modified for Japanese Text Support\n",
    "\n",
    "**MODIFIED**: Added Japanese text normalization testing and enhanced audio generation  \n",
    "**Date**: 2025-07-13  \n",
    "**Purpose**: Test Japanese character processing (hiragana, katakana, kanji) for emotion-controlled TTS  \n",
    "**Changes**: \n",
    "- Added Cell 11: Japanese text processing and emotion control testing\n",
    "- Modified emotion control to work with Japanese text input\n",
    "- Tested normalize_text function with Japanese characters\n",
    "- Enhanced \"Generate samples\" cell (Cell 9) with improved error handling and path management\n",
    "- Added detailed file existence checks and enhanced audio processing workflow\n",
    "- Improved audio enhancement pipeline with better output path handling\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anzua\\Documents\\emoknob\\src\\metavoice-src-main\n",
      "Current working directory after %cd: C:\\Users\\anzua\\Documents\\emoknob\\src\\metavoice-src-main\n",
      "Added C:\\Users\\anzua\\Documents\\emoknob\\src\\metavoice-src-main to sys.path.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# まず、カレントディレクトリを 'src/metavoice-src-main' に変更します。\n",
    "# これにより、以降のパス解決が容易になります。\n",
    "try:\n",
    "    # 既存の %cd がコメントアウトされていても安全に動作するように try-except で囲む\n",
    "    %cd src/metavoice-src-main\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not change directory to src/metavoice-src-main. Error: {e}\")\n",
    "    print(\"Please ensure this directory exists and is correctly specified relative to your notebook's starting directory.\")\n",
    "\n",
    "\n",
    "# 現在のワーキングディレクトリ（変更された後）を取得\n",
    "current_working_directory = os.getcwd()\n",
    "\n",
    "# 確認のため表示\n",
    "print(f\"Current working directory after %cd: {current_working_directory}\")\n",
    "\n",
    "# current_working_directory が fam パッケージの親ディレクトリになるので、\n",
    "# これを sys.path に追加します。\n",
    "if current_working_directory not in sys.path:\n",
    "    sys.path.append(current_working_directory)\n",
    "    print(f\"Added {current_working_directory} to sys.path.\")\n",
    "else:\n",
    "    print(f\"{current_working_directory} is already in sys.path.\")\n",
    "\n",
    "# sys.path の内容をいくつか確認する（デバッグ用、任意）\n",
    "# print(\"\\nUpdated sys.path:\")\n",
    "# for p in sys.path:\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd src/metavoice-src-main\n",
    "\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from fam.llm.adapters import FlattenedInterleavedEncodec2Codebook\n",
    "from fam.llm.decoders import EncodecDecoder\n",
    "from fam.llm.fast_inference_utils import build_model, main\n",
    "from fam.llm.inference import (\n",
    "    EncodecDecoder,\n",
    "    InferenceConfig,\n",
    "    Model,\n",
    "    TiltedEncodec,\n",
    "    TrainedBPETokeniser,\n",
    "    get_cached_embedding,\n",
    "    get_cached_file,    \n",
    "    get_enhancer,\n",
    ")\n",
    "from fam.llm.utils import (\n",
    "    check_audio_file,\n",
    "    get_default_dtype,\n",
    "    get_device,\n",
    "    normalize_text,\n",
    ")\n",
    "\n",
    "model_name = \"metavoiceio/metavoice-1B-v0.1\"\n",
    "seed = 1337\n",
    "output_dir = \"outputs\"\n",
    "_dtype = get_default_dtype()\n",
    "_device = 'cuda:0'\n",
    "_model_dir = snapshot_download(repo_id=model_name)\n",
    "first_stage_adapter = FlattenedInterleavedEncodec2Codebook(end_of_audio_token=1024)\n",
    "output_dir = output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "second_stage_ckpt_path = f\"{_model_dir}/second_stage.pt\"\n",
    "config_second_stage = InferenceConfig(\n",
    "    ckpt_path=second_stage_ckpt_path,\n",
    "    num_samples=1,\n",
    "    seed=seed,\n",
    "    device=_device,\n",
    "    dtype=_dtype,\n",
    "    compile=False,\n",
    "    init_from=\"resume\",\n",
    "    output_dir=output_dir,\n",
    ")\n",
    "data_adapter_second_stage = TiltedEncodec(end_of_audio_token=1024)\n",
    "llm_second_stage = Model(\n",
    "    config_second_stage, TrainedBPETokeniser, EncodecDecoder, data_adapter_fn=data_adapter_second_stage.decode\n",
    ")\n",
    "enhancer = get_enhancer(\"df\")\n",
    "\n",
    "precision = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16}[_dtype]\n",
    "model, tokenizer, smodel, model_size = build_model(\n",
    "    precision=precision,\n",
    "    checkpoint_path=Path(f\"{_model_dir}/first_stage.pt\"),\n",
    "    spk_emb_ckpt_path=Path(f\"{_model_dir}/speaker_encoder.pt\"),\n",
    "    device=_device,\n",
    "    compile=False,\n",
    "    compile_prefill=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain emotion direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: provide your own audio files as (neutral, empathetic) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put in your own audio files as (neutral, empathetic) pairs\n",
    "audio_pairs = [\n",
    "    ('/proj/afosr/metavoice/misc_audio_files/neutral_oprah.wav', '/proj/afosr/metavoice/misc_audio_files/oprah_empathetic_concatenated.wav'),\n",
    "    ('/proj/afosr/metavoice/misc_audio_files/neutral_vt2NjqXKzyA.wav', '/proj/afosr/metavoice/misc_audio_files/vt2NjqXKzyA_empathetic_concatenated.wav')\n",
    "]\n",
    "\n",
    "source_speaker_audio_path = #plug in audio file for voice cloning\n",
    "source_emb = get_cached_embedding(source_speaker_audio_path, smodel).to(device=_device, dtype=precision)\n",
    "\n",
    "speaker_pair_embs = [\n",
    "    (get_cached_embedding(neutral_audio_path, smodel).to(device=_device, dtype=precision), \n",
    "    get_cached_embedding(emotional_audio_path, smodel).to(device=_device, dtype=precision)) for neutral_audio_path, emotional_audio_path in audio_pairs\n",
    "]\n",
    "\n",
    "emo_dirs = [emotional_emb - neutral_emb for neutral_emb, emotional_emb in speaker_pair_embs]\n",
    "emo_dirs = [emo_dir / torch.linalg.norm(emo_dir, dim=-1, keepdim=True) for emo_dir in emo_dirs]\n",
    "\n",
    "emo_dir = sum(emo_dirs) / len(emo_dirs)\n",
    "emo_dir = emo_dir / torch.linalg.norm(emo_dir, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: use our pre-computed emotion directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/afosr/metavoice/final_mv_env/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/proj/afosr/metavoice/emoknob/src/metavoice-src-main\n",
      "available emotions: dict_keys(['charisma', 'empathetic', 'angry', 'contempt', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'desire', 'doubt', 'empathic pain', 'envy', 'joy', 'neutral', 'romance', 'sarcasm', 'tiredness', 'triump'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "emo_dirs = pickle.load(open('../all_emo_dirs.pkl', 'rb'))\n",
    "print(f\"available emotions: {emo_dirs.keys()}\")\n",
    "emo_dir = emo_dirs['sad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strength = 0.3 #set strength of emotion control\n",
    "# 元の英語テキスト\n",
    "#text = \"This is a test.This voice carries the emotion of anger.It may be that it is not possible to create audio longer than two seconds according to the prediction.\"\n",
    "\n",
    "# 日本語対応テキスト\n",
    "text = \"これはテストです。この音声は怒りの感情を持っています。予測によると、2秒より長い音声を作成することは難しいかもしれません。\"\n",
    "\n",
    "# print(f\"使用するテキスト: {text}\")\n",
    "source_speaker_audio_path = #aoudiopath\n",
    "source_emb = get_cached_embedding(source_speaker_audio_path, smodel).to(device=_device, dtype=precision)\n",
    "edited_emb = source_emb + strength * torch.tensor(emo_dir, device=_device, dtype=precision)\n",
    "\n",
    "\n",
    "top_p=0.95\n",
    "guidance_scale=3.0#3.0\n",
    "temperature=1.0\n",
    "text = normalize_text(text)\n",
    "\n",
    "start = time.time()\n",
    "# first stage LLM\n",
    "tokens = main(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_size=model_size,\n",
    "    prompt=text,\n",
    "    spk_emb=edited_emb,\n",
    "    top_p=torch.tensor(top_p, device=_device, dtype=precision),\n",
    "    guidance_scale=torch.tensor(guidance_scale, device=_device, dtype=precision),\n",
    "    temperature=torch.tensor(temperature, device=_device, dtype=precision),\n",
    ")\n",
    "text_ids, extracted_audio_ids = first_stage_adapter.decode([tokens])\n",
    "\n",
    "b_speaker_embs = edited_emb.unsqueeze(0)\n",
    "\n",
    "# second stage LLM + multi-band diffusion model\n",
    "wav_files = llm_second_stage(\n",
    "    texts=[text],\n",
    "    encodec_tokens=[torch.tensor(extracted_audio_ids, dtype=torch.int32, device=_device).unsqueeze(0)],\n",
    "    speaker_embs=b_speaker_embs,\n",
    "    batch_size=1,\n",
    "    guidance_scale=None,\n",
    "    top_p=None,\n",
    "    top_k=200,\n",
    "    temperature=1.0,\n",
    "    max_new_tokens=None,\n",
    ")\n",
    "print(f\"wav_files: {wav_files}\")\n",
    "wav_file = wav_files[0]\n",
    "generated_raw_audio_path = str(wav_file) + \".wav\"\n",
    "# if not os.path.exists(full_wav_path):\n",
    "#     print(f\"Error: Audio file not found at {full_wav_path}\")\n",
    "# else:\n",
    "#     with tempfile.NamedTemporaryFile(suffix=\".wav\") as enhanced_tmp:\n",
    "#         enhancer(str(wav_file) + \".wav\", enhanced_tmp.name)\n",
    "#         shutil.copy2(enhanced_tmp.name, str(wav_file) + \".wav\")\n",
    "#         print(f\"\\nSaved audio to {wav_file}.wav\")\n",
    "\n",
    "# output_path = str(wav_file) + \".wav\"\n",
    "\n",
    "\n",
    "# # Display the generated audio\n",
    "# from IPython.display import Audio, display\n",
    "\n",
    "# display(Audio(output_path))\n",
    "\n",
    "# 強化後のオーディオの出力パスを定義\n",
    "# 例: 元のファイル名に \"_enhanced.wav\" をつける\n",
    "# Path(generated_raw_audio_path).parent は元のファイルのディレクトリ\n",
    "# Path(generated_raw_audio_path).stem は拡張子を含まないファイル名\n",
    "enhanced_output_filename = Path(generated_raw_audio_path).stem + \"_enhanced.wav\"\n",
    "# output_dir は既に定義されているはずの変数（例: \"outputs\"）\n",
    "# final_enhanced_path = Path(output_dir) / enhanced_output_filename # これは元の output_dir に保存\n",
    "# または、元の生成されたファイルのディレクトリに保存するなら\n",
    "final_enhanced_path = Path(generated_raw_audio_path).parent / enhanced_output_filename\n",
    "\n",
    "\n",
    "print(f\"Checking if raw audio exists: {generated_raw_audio_path}\")\n",
    "if not os.path.exists(generated_raw_audio_path):\n",
    "    print(f\"Error: Raw audio file for enhancement not found at {generated_raw_audio_path}\")\n",
    "    print(\"Please check if the first stage LLM correctly generated and saved the file.\")\n",
    "else:\n",
    "    print(f\"Raw audio found. Enhancing and saving to {final_enhanced_path}\")\n",
    "    try:\n",
    "        # enhancer を使って、生成された生のオーディオを読み込み、強化して、新しいパスに保存する\n",
    "        # enhancer のインターフェースが (input_path, output_path) であることを前提\n",
    "        enhancer(generated_raw_audio_path, str(final_enhanced_path))\n",
    "\n",
    "        # shutil.copy2 は enhancer が直接出力しない場合にのみ必要\n",
    "        # ここでは enhancer が直接出力することを期待\n",
    "        print(f\"\\nSaved enhanced audio to {final_enhanced_path}\")\n",
    "        output_path = str(final_enhanced_path) # 表示用のパスを更新\n",
    "\n",
    "        # Display the generated audio\n",
    "        from IPython.display import Audio, display\n",
    "        display(Audio(output_path))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during enhancement and saving: {e}\")\n",
    "        # 追加のデバッグ情報\n",
    "        if \"System error\" in str(e) and os.path.exists(generated_raw_audio_path):\n",
    "            print(\"This suggests an issue with writing the *enhanced* file, not reading the raw one.\")\n",
    "            print(\"Possible causes: permissions for the output directory, disk space, or a problem with the soundfile/libsndfile library itself trying to create the file.\")\n",
    "        elif \"No such file or directory\" in str(e):\n",
    "             print(f\"The input file '{generated_raw_audio_path}' might not actually exist yet or something else is wrong.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語テキストでの音声合成テスト（感情制御付き）\n",
    "print(\"=== 日本語音声合成 + 感情制御テスト ===\")\n",
    "\n",
    "# 日本語テキストの設定\n",
    "japanese_test_text = \"これはテストです。この音声は怒りの感情を持っています。予測によると、2秒より長い音声を作成することは難しいかもしれません。\"\n",
    "\n",
    "try:\n",
    "    # Step 1: テキスト正規化のテスト\n",
    "    normalized_text = normalize_text(japanese_test_text)\n",
    "    print(f\"Step 1: 日本語テキスト正規化成功\")\n",
    "    print(f\"   元のテキスト: {japanese_test_text}\")\n",
    "    print(f\"   正規化後: {normalized_text}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: 感情制御の準備\n",
    "    print(\"Step 2: 感情制御の準備\")\n",
    "    strength = 0.3\n",
    "    emotion = 'angry'  # 怒りの感情を使用\n",
    "    emo_dir = emo_dirs[emotion]\n",
    "    print(f\"   感情: {emotion}\")\n",
    "    print(f\"   強度: {strength}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 3: スピーカーエンベッディングの設定\n",
    "    print(\"Step 3: スピーカーエンベッディングと感情制御\")\n",
    "    source_speaker_audio_path = \"C:/Users/anzua/Documents/emoknob/docs/audios/simple_emotion_emotext/angry_0.0_1_emotext0_MSP.wav\"\n",
    "    source_emb = get_cached_embedding(source_speaker_audio_path, smodel).to(device=_device, dtype=precision)\n",
    "    edited_emb = source_emb + strength * torch.tensor(emo_dir, device=_device, dtype=precision)\n",
    "    print(f\"   ソースエンベッディング形状: {source_emb.shape}\")\n",
    "    print(f\"   感情制御適用後の形状: {edited_emb.shape}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"日本語テキスト処理と感情制御の準備が完了しました！\")\n",
    "    print(\"注意: 実際の音声生成には、BPEトークナイザーの日本語語彙対応が必要です。\")\n",
    "    print(\"   テキスト正規化と感情制御機能は正常に動作しています。\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"エラーが発生しました: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
